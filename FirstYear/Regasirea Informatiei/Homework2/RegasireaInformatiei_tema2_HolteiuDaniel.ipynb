{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RegasireaInformatiei-tema2-HolteiuDaniel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLqx1Dpsk2dA",
        "colab_type": "code",
        "outputId": "a26fff50-3cae-443d-aff5-5e28727e97d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Imports\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.preprocessing import text\n",
        "from keras import utils\n",
        "\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAJThbmqtaKS",
        "colab_type": "code",
        "outputId": "df5b45b0-7a0b-4f3b-fa5a-49434153b65a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 57
        }
      },
      "source": [
        "# import files\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-217b9ce2-6293-4483-9dcd-6a085c7eaf9f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-217b9ce2-6293-4483-9dcd-6a085c7eaf9f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KROZCO8k-46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# constants\n",
        "TRAIN_DATA_FILE_PATH = \"Lyrics-Genre-Train.csv\"\n",
        "TEST_DATA_FILE_PATH = \"Lyrics-Genre-Test-GroundTruth.csv\"\n",
        "LYRICS_COLUMN = \"Lyrics\"\n",
        "GENRE_COLUMN = \"Genre\"\n",
        "\n",
        "# useful variables\n",
        "output_mappings = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2-Aaz1UlDpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# citeste datele din fisier\n",
        "def read_data(file_path):\n",
        "    return pd.read_csv(file_path)[[LYRICS_COLUMN, GENRE_COLUMN]]\n",
        "\n",
        "\n",
        "# clean la input si mapare la int pentru output\n",
        "def create_new_dataset(dataset):\n",
        "    global output_mappings\n",
        "    dataset[GENRE_COLUMN] = dataset[GENRE_COLUMN].map(output_mappings)\n",
        "    dataset[LYRICS_COLUMN] = dataset[LYRICS_COLUMN].apply(clean_text)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# aplicarea de create_new_dataset\n",
        "def label_encoding(train_data, test_data):\n",
        "    possible_labels = train_data[GENRE_COLUMN].unique()\n",
        "    global output_mappings\n",
        "    output_mappings = {genre: index for index, genre in enumerate(possible_labels)}\n",
        "    new_train_data = create_new_dataset(train_data)\n",
        "    new_test_data = create_new_dataset(test_data)\n",
        "    return new_train_data, new_test_data\n",
        "  \n",
        "\n",
        "# cleaning text - o recomandare de la google\n",
        "def clean_text(text):\n",
        "  REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]!.')\n",
        "  BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "  STOPWORDS = set(stopwords.words('english'))\n",
        "  \n",
        "  text = BeautifulSoup(text, \"lxml\").text\n",
        "  text = text.lower() \n",
        "  text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "  text = BAD_SYMBOLS_RE.sub('', text) \n",
        "  text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "  return text\n",
        "    \n",
        "  \n",
        "# preia doar un procentaj din date - folosit pentru cazurile in care antrenarea dureaza foarte mult\n",
        "def split_data(data, percentage):\n",
        "  if (percentage == 1.0):\n",
        "    return data\n",
        "  data_unique = data[GENRE_COLUMN].unique()\n",
        "  new_data = pd.DataFrame(columns=[LYRICS_COLUMN, GENRE_COLUMN])\n",
        "  for i in data_unique:\n",
        "    aux = data.loc[data[GENRE_COLUMN] == i]\n",
        "    aux = aux.head(int(len(aux) * percentage))\n",
        "    new_data = new_data.append(aux)\n",
        "  \n",
        "  return new_data\n",
        "    \n",
        "\n",
        "# split intre input si output (versuri si gen)\n",
        "def split_input_label(data):\n",
        "  x = data[LYRICS_COLUMN]\n",
        "  y = data[GENRE_COLUMN]\n",
        "  y.astype('int')\n",
        "  return x, y\n",
        "\n",
        "\n",
        "# TFIDF vectorizer\n",
        "def tf_idf(train_data, test_data):\n",
        "    vectorizer = TfidfVectorizer(strip_accents='ascii', stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
        "\n",
        "    all_data = train_data.append(test_data)\n",
        "    all_data = vectorizer.fit_transform(all_data[LYRICS_COLUMN])\n",
        "    x_train = all_data[0:train_data.shape[0]]\n",
        "    x_test = all_data[train_data.shape[0]:]\n",
        "    \n",
        "    y_train = train_data[GENRE_COLUMN]\n",
        "    y_train.astype('int')\n",
        "    y_test = test_data[GENRE_COLUMN]\n",
        "    y_test.astype('int')\n",
        "    return x_train, y_train, x_test, y_test\n",
        "  \n",
        "# SVC model\n",
        "def SVC_model(x_train, y_train, x_test, y_test):\n",
        "  svc_model = SVC(C=1.0, gamma=1.0, kernel='linear')\n",
        "  scores = cross_val_score(svc_model, x_train, y_train, cv=5)\n",
        "  print(scores)\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "  print(\"--------------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"Training on all data and calculating accuracy on test data\")\n",
        "  svc_model = SVC(C=1.0, gamma=1.0, kernel='linear')\n",
        "  svc_model.fit(x_train, y_train)\n",
        "  accuracy = accuracy_score(y_test, svc_model.predict(x_test))\n",
        "  precision = precision_score(y_test, svc_model.predict(x_test), average=\"weighted\")\n",
        "  f1_score_ = f1_score(y_test, svc_model.predict(x_test), average=\"weighted\")\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"f1 score:\", f1_score_)\n",
        "  \n",
        "\n",
        "# MultinomialNB model\n",
        "def MultinomialNB_model(x_train, y_train, x_test, y_test):\n",
        "  multinominalnb_model = MultinomialNB()\n",
        "  scores = cross_val_score(multinominalnb_model, x_train, y_train, cv=5)\n",
        "  print(scores)\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "  print(\"--------------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"Training on all data and calculating accuracy on test data\")\n",
        "  multinominalnb_model = MultinomialNB()\n",
        "  multinominalnb_model.fit(x_train, y_train)\n",
        "  accuracy = accuracy_score(y_test, multinominalnb_model.predict(x_test))\n",
        "  precision = precision_score(y_test, multinominalnb_model.predict(x_test), average=\"weighted\")\n",
        "  f1_score_ = f1_score(y_test, multinominalnb_model.predict(x_test), average=\"weighted\")\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"f1 score:\", f1_score_)\n",
        "  \n",
        "\n",
        "# SGD Classifier model\n",
        "def SGDClassifier_model(x_train, y_train, x_test, y_test):\n",
        "  sgdclassifier_model = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
        "  scores = cross_val_score(sgdclassifier_model, x_train, y_train, cv=5)\n",
        "  print(scores)\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "  print(\"--------------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"Training on all data and calculating accuracy on test data\")\n",
        "  sgdclassifier_model =  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
        "  sgdclassifier_model.fit(x_train, y_train)\n",
        "  accuracy = accuracy_score(y_test, sgdclassifier_model.predict(x_test))\n",
        "  precision = precision_score(y_test, sgdclassifier_model.predict(x_test), average=\"weighted\")\n",
        "  f1_score_ = f1_score(y_test, sgdclassifier_model.predict(x_test), average=\"weighted\")\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"f1 score:\", f1_score_)\n",
        "  \n",
        "  \n",
        "# Logistic Regression model\n",
        "def LogisticRegression_model(x_train, y_train, x_test, y_test):\n",
        "  logisticregression_model = LogisticRegression(n_jobs=1, C=1e5)\n",
        "  scores = cross_val_score(logisticregression_model, x_train, y_train, cv=5)\n",
        "  print(scores)\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "  print(\"--------------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"Training on all data and calculating accuracy on test data\")\n",
        "  logisticregression_model = LogisticRegression(n_jobs=1, C=1e5)\n",
        "  logisticregression_model.fit(x_train, y_train)\n",
        "  accuracy = accuracy_score(y_test, logisticregression_model.predict(x_test))\n",
        "  precision = precision_score(y_test, logisticregression_model.predict(x_test), average=\"weighted\")\n",
        "  f1_score_ = f1_score(y_test, logisticregression_model.predict(x_test), average=\"weighted\")\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"f1 score:\", f1_score_)\n",
        "  \n",
        "  \n",
        "# MultinomialNB model using count vectorizer and TFIDF transformer\n",
        "def count_vectorizer_tfidf_multinominalnb(x_train, y_train, x_test, y_test):\n",
        "  pipe = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "\n",
        "  scores = cross_val_score(pipe, x_train, y_train, cv=5)\n",
        "  print(scores)\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "  print(\"--------------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"Training on all data and calculating accuracy on test data\")\n",
        "  \n",
        "  pipe = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "  \n",
        "  pipe.fit(x_train, y_train)\n",
        "  accuracy = accuracy_score(y_test, pipe.predict(x_test))\n",
        "  precision = precision_score(y_test, pipe.predict(x_test), average=\"weighted\")\n",
        "  f1_score_ = f1_score(y_test, pipe.predict(x_test), average=\"weighted\")\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"f1 score:\", f1_score_)\n",
        "  \n",
        "  \n",
        "# SGD Classifier model using count vectorizer and TFIDF transformer\n",
        "def count_vectorizer_tfidf_sgd_classifier(x_train, y_train, x_test, y_test):\n",
        "  pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])\n",
        "\n",
        "  scores = cross_val_score(pipe, x_train, y_train, cv=5)\n",
        "  print(scores)\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "  print(\"--------------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"Training on all data and calculating accuracy on test data\")\n",
        "  \n",
        "  pipe = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "  \n",
        "  pipe.fit(x_train, y_train)\n",
        "  accuracy = accuracy_score(y_test, pipe.predict(x_test))\n",
        "  precision = precision_score(y_test, pipe.predict(x_test), average=\"weighted\")\n",
        "  f1_score_ = f1_score(y_test, pipe.predict(x_test), average=\"weighted\")\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"f1 score:\", f1_score_)\n",
        "  \n",
        "  \n",
        "# Logistic Regression model using count vectorizer and TFIDF transformer\n",
        "def count_vectorizer_tfidf_logistic_regression(x_train, y_train, x_test, y_test):\n",
        "  pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
        "               ])\n",
        "\n",
        "  scores = cross_val_score(pipe, x_train, y_train, cv=5)\n",
        "  print(scores)\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
        "  print(\"--------------------------------------------------------------------------------------------------------\")\n",
        "  print()\n",
        "  print(\"Training on all data and calculating accuracy on test data\")\n",
        "  \n",
        "  pipe = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "  \n",
        "  pipe.fit(x_train, y_train)\n",
        "  accuracy = accuracy_score(y_test, pipe.predict(x_test))\n",
        "  precision = precision_score(y_test, pipe.predict(x_test), average=\"weighted\")\n",
        "  f1_score_ = f1_score(y_test, pipe.predict(x_test), average=\"weighted\")\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Precision:\", precision)\n",
        "  print(\"f1 score:\", f1_score_)\n",
        "  \n",
        "\n",
        "  \n",
        "# Keras model using Bag of Words from keras\n",
        "def keras_model_bow(all_data, train_data, test_data, loss_function):\n",
        "  max_words = 1024\n",
        "  tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
        "  tokenize.fit_on_texts(all_data[LYRICS_COLUMN])\n",
        "\n",
        "  x_train = tokenize.texts_to_matrix(train_data[LYRICS_COLUMN])\n",
        "  x_test = tokenize.texts_to_matrix(test_data[LYRICS_COLUMN])\n",
        "\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(all_data[GENRE_COLUMN])\n",
        "  y_train = encoder.transform(train_data[GENRE_COLUMN])\n",
        "  y_test = encoder.transform(test_data[GENRE_COLUMN])\n",
        "\n",
        "  num_classes = np.max(y_train) + 1\n",
        "  y_train = utils.to_categorical(y_train, num_classes)\n",
        "  y_test = utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "  batch_size = 32\n",
        "  epochs = 5\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(512, input_shape=(max_words,)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  model.compile(loss=loss_function,\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      verbose=1,\n",
        "                      validation_split=0.1)\n",
        "    \n",
        "  score = model.evaluate(x_test, y_test,\n",
        "                     batch_size=batch_size, verbose=1)\n",
        "  print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxdFJqY0lITR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = read_data(TRAIN_DATA_FILE_PATH)\n",
        "test_data = read_data(TEST_DATA_FILE_PATH)\n",
        "\n",
        "# Aleg doar un procentaj mic din date in cazul modelelor care necesita mult prea mult timp de antrenare (SVC)\n",
        "# Astfel, rezultatele nu vor fi \"reale\" intrucat nu se folosesc toate datele.\n",
        "train_data = split_data(train_data, 0.3)\n",
        "test_data = split_data(test_data, 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU3CF2ALmgSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encodez label-urile cu int-uri de la 0 la 9 in toat cazurile\n",
        "train_data, test_data = label_encoding(train_data, test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6P4YkBe6Ir",
        "colab_type": "text"
      },
      "source": [
        "Voi extrage feature-urile in diferite moduri la care voi aplica diferiti algoritmi de invatare. \n",
        "\n",
        "Pentru fiecare model ales voi face cross validation.\n",
        "\n",
        "Dupa rezultatele din cross validation voi calcula acuratetea pe datele de test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myoC7VG1gkYf",
        "colab_type": "text"
      },
      "source": [
        "Voi incepe prin extragerea feature-urilor folosind TFIDF si incercarea de diferiti algoritmi de invatare\n",
        "\n",
        "Mentionez ca in fiecare code zone exista un comment care spune ce algoritm de invatare am folosit si ce feature extraction am folosit.\n",
        "\n",
        "Rezultatele sunt afisate dupa fiecare rulare de algoritm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvwI1RXJmkQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_tfidf, y_train_tfidf, x_test_tfidf, y_test_tfidf = tf_idf(train_data, test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCw9ctchisRH",
        "colab_type": "code",
        "outputId": "99ca08a6-2ddd-4bec-9a83-9ae3fe9852f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# SVC + tfidf\n",
        "SVC_model(x_train_tfidf, y_train_tfidf, x_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.35483871 0.33154122 0.34146341 0.34688347 0.31526649]\n",
            "Accuracy: 0.34 (+/- 0.03)\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Training on all data and calculating accuracy on test data\n",
            "Accuracy: 0.35714285714285715\n",
            "Precision: 0.4424175596973597\n",
            "f1 score: 0.3185853823831478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuThG9Io3I2v",
        "colab_type": "code",
        "outputId": "88553336-d529-4f43-bb9b-d9a65e58a395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# MultinomialNB_model + tfidf\n",
        "train_data = read_data(TRAIN_DATA_FILE_PATH)\n",
        "test_data = read_data(TEST_DATA_FILE_PATH)\n",
        "train_data_all = split_data(train_data, 1.0)\n",
        "test_data_all = split_data(test_data, 1.0)\n",
        "train_data, test_data = label_encoding(train_data, test_data)\n",
        "x_train_tfidf, y_train_tfidf, x_test_tfidf, y_test_tfidf = tf_idf(train_data, test_data)\n",
        "\n",
        "MultinomialNB_model(x_train_tfidf, y_train_tfidf, x_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.28220362 0.28058331 0.28274372 0.28227985 0.28119935]\n",
            "Accuracy: 0.28 (+/- 0.00)\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Training on all data and calculating accuracy on test data\n",
            "Accuracy: 0.28733459357277885\n",
            "Precision: 0.4694960572492441\n",
            "f1 score: 0.18698440442165717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuRSGCX5tSzS",
        "colab_type": "code",
        "outputId": "b9d696fc-0224-43b5-ade3-03d272ddb324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# SGDClassifier model + tfidf\n",
        "train_data = read_data(TRAIN_DATA_FILE_PATH)\n",
        "test_data = read_data(TEST_DATA_FILE_PATH)\n",
        "train_data_all = split_data(train_data, 1.0)\n",
        "test_data_all = split_data(test_data, 1.0)\n",
        "train_data, test_data = label_encoding(train_data, test_data)\n",
        "x_train_tfidf, y_train_tfidf, x_test_tfidf, y_test_tfidf = tf_idf(train_data, test_data)\n",
        "\n",
        "SGDClassifier_model(x_train_tfidf, y_train_tfidf, x_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.37186065 0.36402917 0.37456117 0.37385197 0.37736359]\n",
            "Accuracy: 0.37 (+/- 0.01)\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Training on all data and calculating accuracy on test data\n",
            "Accuracy: 0.3784499054820416\n",
            "Precision: 0.34919864283879115\n",
            "f1 score: 0.3448792972018557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkZvBLSEvVH0",
        "colab_type": "code",
        "outputId": "d7d17c61-b062-4f7d-b436-2f31a65754c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# LogisticRegression model + tfidf\n",
        "train_data = read_data(TRAIN_DATA_FILE_PATH)\n",
        "test_data = read_data(TEST_DATA_FILE_PATH)\n",
        "train_data_all = split_data(train_data, 1.0)\n",
        "test_data_all = split_data(test_data, 1.0)\n",
        "train_data, test_data = label_encoding(train_data, test_data)\n",
        "x_train_tfidf, y_train_tfidf, x_test_tfidf, y_test_tfidf = tf_idf(train_data, test_data)\n",
        "\n",
        "LogisticRegression_model(x_train_tfidf, y_train_tfidf, x_test_tfidf, y_test_tfidf)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.36834999 0.36132865 0.38968404 0.37520259 0.3719611 ]\n",
            "Accuracy: 0.37 (+/- 0.02)\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Training on all data and calculating accuracy on test data\n",
            "Accuracy: 0.3696282293635791\n",
            "Precision: 0.3631043741302422\n",
            "f1 score: 0.361945715979274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qWhjeusyfGH",
        "colab_type": "text"
      },
      "source": [
        "Rezultatele de mai sus nu sunt foarte promitatoare intrucat au o acuratete foarte mica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9lqZlrOzvro",
        "colab_type": "text"
      },
      "source": [
        "Urmatorul aproach va fi Count Vectorizer + TFIDF Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ2gYHISyl6I",
        "colab_type": "code",
        "outputId": "80b2229f-36bf-4757-d75b-c449df3aa1f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# MultinomialNB_model + count vectorizer + tfidf\n",
        "\n",
        "train_data = read_data(TRAIN_DATA_FILE_PATH)\n",
        "test_data = read_data(TEST_DATA_FILE_PATH)\n",
        "train_data_all = split_data(train_data, 1.0)\n",
        "test_data_all = split_data(test_data, 1.0)\n",
        "train_data, test_data = label_encoding(train_data, test_data)\n",
        "\n",
        "x_train, y_train = split_input_label(train_data)\n",
        "x_test, y_test = split_input_label(test_data)\n",
        "\n",
        "count_vectorizer_tfidf_multinominalnb(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.28193357 0.28436403 0.28679449 0.28417072 0.28444084]\n",
            "Accuracy: 0.28 (+/- 0.00)\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Training on all data and calculating accuracy on test data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2882167611846251\n",
            "Precision: 0.37774713374138685\n",
            "f1 score: 0.18772251264119305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oTuPROj0y-7",
        "colab_type": "code",
        "outputId": "a624cd03-1a8f-4ce9-b98d-dfec1d1be995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# sgd classifier + count vectorizer + tfidf\n",
        "\n",
        "train_data = read_data(TRAIN_DATA_FILE_PATH)\n",
        "test_data = read_data(TEST_DATA_FILE_PATH)\n",
        "train_data_all = split_data(train_data, 1.0)\n",
        "test_data_all = split_data(test_data, 1.0)\n",
        "train_data, test_data = label_encoding(train_data, test_data)\n",
        "\n",
        "x_train, y_train = split_input_label(train_data)\n",
        "x_test, y_test = split_input_label(test_data)\n",
        "\n",
        "count_vectorizer_tfidf_sgd_classifier(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.36591952 0.35862814 0.36943019 0.36088601 0.36763911]\n",
            "Accuracy: 0.36 (+/- 0.01)\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Training on all data and calculating accuracy on test data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2882167611846251\n",
            "Precision: 0.37774713374138685\n",
            "f1 score: 0.18772251264119305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwl1LkaIBK2Q",
        "colab_type": "code",
        "outputId": "516462fb-4766-40a6-fa32-8db96b496435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# logistic regression + count vectorizer + tfidf\n",
        "\n",
        "train_data = read_data(TRAIN_DATA_FILE_PATH)\n",
        "test_data = read_data(TEST_DATA_FILE_PATH)\n",
        "train_data_all = split_data(train_data, 1.0)\n",
        "test_data_all = split_data(test_data, 1.0)\n",
        "train_data, test_data = label_encoding(train_data, test_data)\n",
        "\n",
        "x_train, y_train = split_input_label(train_data)\n",
        "x_test, y_test = split_input_label(test_data)\n",
        "\n",
        "count_vectorizer_tfidf_logistic_regression(x_train, y_train, x_test, y_test)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.36402917 0.36753983 0.38536322 0.37250135 0.36925986]\n",
            "Accuracy: 0.37 (+/- 0.01)\n",
            "--------------------------------------------------------------------------------------------------------\n",
            "\n",
            "Training on all data and calculating accuracy on test data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.2882167611846251\n",
            "Precision: 0.37774713374138685\n",
            "f1 score: 0.18772251264119305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QumPDFzqnRLr",
        "colab_type": "text"
      },
      "source": [
        "Ultimul aproach va fi folosirea unui neural network din keras si varianta de bag of words tot din keras folosind diferite functii de loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScC5Z2R2n5hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data for keras model\n",
        "\n",
        "train_data = read_data(TRAIN_DATA_FILE_PATH)\n",
        "test_data = read_data(TEST_DATA_FILE_PATH)\n",
        "train_data_all = split_data(train_data, 1.0)\n",
        "test_data_all = split_data(test_data, 1.0)\n",
        "train_data, test_data = label_encoding(train_data, test_data)\n",
        "all_data = train_data.append(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AdWZQHGn_21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3a17822d-3c45-452e-b93f-61d7994e1a8d"
      },
      "source": [
        "# Keras + Bow + categorical_crossentropy\n",
        "keras_model_bow(all_data, train_data, test_data, \"categorical_crossentropy\")"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16661 samples, validate on 1852 samples\n",
            "Epoch 1/5\n",
            "16661/16661 [==============================] - 7s 444us/step - loss: 1.8905 - acc: 0.3352 - val_loss: 1.7557 - val_acc: 0.3969\n",
            "Epoch 2/5\n",
            "16661/16661 [==============================] - 5s 303us/step - loss: 1.6018 - acc: 0.4460 - val_loss: 1.7498 - val_acc: 0.3909\n",
            "Epoch 3/5\n",
            "16661/16661 [==============================] - 5s 310us/step - loss: 1.4171 - acc: 0.5126 - val_loss: 1.7785 - val_acc: 0.3866\n",
            "Epoch 4/5\n",
            "16661/16661 [==============================] - 5s 299us/step - loss: 1.2315 - acc: 0.5852 - val_loss: 1.8278 - val_acc: 0.3898\n",
            "Epoch 5/5\n",
            "16661/16661 [==============================] - 5s 302us/step - loss: 1.0471 - acc: 0.6535 - val_loss: 1.8841 - val_acc: 0.3866\n",
            "7935/7935 [==============================] - 1s 71us/step\n",
            "Test accuracy: 0.37328292369542065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z_jNFKvoPUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "508f640d-8713-450f-a159-2f16c50ad966"
      },
      "source": [
        "# Keras + Bow + kullback_leibler_divergence\n",
        "keras_model_bow(all_data, train_data, test_data, \"kullback_leibler_divergence\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16661 samples, validate on 1852 samples\n",
            "Epoch 1/5\n",
            "16661/16661 [==============================] - 7s 437us/step - loss: 1.8931 - acc: 0.3317 - val_loss: 1.7536 - val_acc: 0.3812\n",
            "Epoch 2/5\n",
            "16661/16661 [==============================] - 5s 297us/step - loss: 1.5995 - acc: 0.4445 - val_loss: 1.7459 - val_acc: 0.4060\n",
            "Epoch 3/5\n",
            "16661/16661 [==============================] - 5s 294us/step - loss: 1.4114 - acc: 0.5157 - val_loss: 1.7833 - val_acc: 0.3861\n",
            "Epoch 4/5\n",
            "16661/16661 [==============================] - 5s 298us/step - loss: 1.2223 - acc: 0.5921 - val_loss: 1.8250 - val_acc: 0.3888\n",
            "Epoch 5/5\n",
            "16661/16661 [==============================] - 5s 300us/step - loss: 1.0408 - acc: 0.6575 - val_loss: 1.8846 - val_acc: 0.3866\n",
            "7935/7935 [==============================] - 1s 71us/step\n",
            "Test accuracy: 0.36584751098202545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHdDW33voP9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "553b3a31-6ac8-44bf-b595-fcaf88d4928f"
      },
      "source": [
        "# Keras + Bow + poisson\n",
        "keras_model_bow(all_data, train_data, test_data, \"poisson\")"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16661 samples, validate on 1852 samples\n",
            "Epoch 1/5\n",
            "16661/16661 [==============================] - 8s 450us/step - loss: 0.2893 - acc: 0.3314 - val_loss: 0.2748 - val_acc: 0.3877\n",
            "Epoch 2/5\n",
            "16661/16661 [==============================] - 5s 304us/step - loss: 0.2600 - acc: 0.4460 - val_loss: 0.2738 - val_acc: 0.4001\n",
            "Epoch 3/5\n",
            "16661/16661 [==============================] - 5s 308us/step - loss: 0.2419 - acc: 0.5137 - val_loss: 0.2769 - val_acc: 0.3882\n",
            "Epoch 4/5\n",
            "16661/16661 [==============================] - 5s 305us/step - loss: 0.2223 - acc: 0.5875 - val_loss: 0.2825 - val_acc: 0.3866\n",
            "Epoch 5/5\n",
            "16661/16661 [==============================] - 5s 310us/step - loss: 0.2043 - acc: 0.6561 - val_loss: 0.2893 - val_acc: 0.3807\n",
            "7935/7935 [==============================] - 1s 73us/step\n",
            "Test accuracy: 0.3692501574773494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbxzALj3oRMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e01036f1-37ff-48b0-d767-4fe3975cdfb0"
      },
      "source": [
        "# Keras + Bow + cosine_proximity\n",
        "keras_model_bow(all_data, train_data, test_data, \"cosine_proximity\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16661 samples, validate on 1852 samples\n",
            "Epoch 1/5\n",
            "16661/16661 [==============================] - 8s 460us/step - loss: -0.4464 - acc: 0.3349 - val_loss: -0.4921 - val_acc: 0.3915\n",
            "Epoch 2/5\n",
            "16661/16661 [==============================] - 5s 320us/step - loss: -0.5333 - acc: 0.4545 - val_loss: -0.4966 - val_acc: 0.4044\n",
            "Epoch 3/5\n",
            "16661/16661 [==============================] - 5s 306us/step - loss: -0.5915 - acc: 0.5378 - val_loss: -0.4926 - val_acc: 0.3969\n",
            "Epoch 4/5\n",
            "16661/16661 [==============================] - 5s 307us/step - loss: -0.6443 - acc: 0.6103 - val_loss: -0.4848 - val_acc: 0.3942\n",
            "Epoch 5/5\n",
            "16661/16661 [==============================] - 5s 300us/step - loss: -0.6942 - acc: 0.6738 - val_loss: -0.4822 - val_acc: 0.3904\n",
            "7935/7935 [==============================] - 1s 74us/step\n",
            "Test accuracy: 0.36899810958449464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDU7vGUNoRVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4d3ef1bb-e1e9-4f41-c203-50cbac49b321"
      },
      "source": [
        "# Keras + Bow + binary_crossentropy\n",
        "keras_model_bow(all_data, train_data, test_data, \"binary_crossentropy\")"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 16661 samples, validate on 1852 samples\n",
            "Epoch 1/5\n",
            "16661/16661 [==============================] - 8s 459us/step - loss: 0.2752 - acc: 0.9081 - val_loss: 0.2582 - val_acc: 0.9106\n",
            "Epoch 2/5\n",
            "16661/16661 [==============================] - 5s 306us/step - loss: 0.2394 - acc: 0.9130 - val_loss: 0.2568 - val_acc: 0.9110\n",
            "Epoch 3/5\n",
            "16661/16661 [==============================] - 5s 315us/step - loss: 0.2156 - acc: 0.9195 - val_loss: 0.2616 - val_acc: 0.9086\n",
            "Epoch 4/5\n",
            "16661/16661 [==============================] - 5s 319us/step - loss: 0.1915 - acc: 0.9278 - val_loss: 0.2703 - val_acc: 0.9076\n",
            "Epoch 5/5\n",
            "16661/16661 [==============================] - 5s 313us/step - loss: 0.1673 - acc: 0.9373 - val_loss: 0.2785 - val_acc: 0.9056\n",
            "7935/7935 [==============================] - 1s 74us/step\n",
            "Test accuracy: 0.9051291559654433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sjHCGjj0uSu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Avand in vedere rezultatele combinatiilor de mai sus putem concluziona ca cele mai bune rezultate au fost obtinute de modelul Keras folosind ca loss function Binary Crossentropy"
      ]
    }
  ]
}