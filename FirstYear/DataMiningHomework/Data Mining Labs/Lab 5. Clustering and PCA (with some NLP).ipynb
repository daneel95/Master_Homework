{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Lab_5_Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "QbJto_4BzREm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 5. Natural Language Processing. Unsupervised Learning"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ka3g6qXCcZ3_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Some IPython magic\n",
        "# Put these at the top of every notebook, here nbagg is used for interactive plots\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JBns_Q4qY2Fx"
      },
      "cell_type": "markdown",
      "source": [
        "## Natural Language Processing.\n",
        "NLP refers to processing text data. This could refer to a wide range of tasks, from very simple ones, like searching for a pattern, to very complex ones, like text summarization, or automated translation.\n",
        "\n",
        "### Feature Extraction\n",
        "In order to apply Machine Learning algorithms on text data, we need to figure out a way to represent the text as a set of numeric attributes.\n",
        "\n",
        "#### Bag of Words\n",
        "The simplest way to represent a text document as a vector of numbers is to count the words, and output a frequency count. Let's say we have a list of all english words, like the following:"
      ]
    },
    {
      "metadata": {
        "id": "aaExhqt5zRE0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# creates a wordlist, with all words, from \"a\" to \"zygote\"\n",
        "import urllib.request as request\n",
        "words = request.urlopen(\"https://svnweb.freebsd.org/csrg/share/dict/words?view=co\")\n",
        "wordlist = []\n",
        "for w in words:\n",
        "    wordlist.append(str(w.decode().strip()))\n",
        "print(', '.join(wordlist[:4]) + \" ... \" + ', '.join(wordlist[-4:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgVJ47zczRE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Now we can convert any text to a vector of size \" \\\n",
        "      + str(len(wordlist)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z4WzacIpzRFC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, the text \"In this lab we study Natural Language Processing and Unsupervised Learning\" can be represented as a vector with almost all values equal to 0, and values of 1 in the position of the words \"in\", \"this\", etc.\n",
        "\n",
        "This ___feature vector___ can be extracted directly from the dataset. If we have a large collection of text, we can assume that other documents will use the same vocabulary. So if we build a model for news articles, most likely those articles will not use every single word in the english language. So during the training phase of our machine learning modeling, we can use the train set to create our feature vector, we select only the words that appear in the train set. If new words appear during the test phase, we will discard them. This is a good thing to do because during training, we did not learn anything about those words. We cannot use unseen words to perform classification.\n",
        "\n",
        "Let's start working with a dataset."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-BieAe7lWcWg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# We select only 3 categories for now, feel free to change the categories\n",
        "categories = [\n",
        "    'rec.sport.baseball',\n",
        "    'comp.graphics',\n",
        "    'sci.space',\n",
        "]\n",
        "dataset = fetch_20newsgroups(subset='all', categories=categories, \n",
        "                              shuffle=True, random_state=42)\n",
        "\n",
        "#dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=42) #if you want all caterogies\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7pwUTmInbViU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# here are the attributes of the object retrieved by fetch_20newsgroups\n",
        "dir(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_VKru4wJbHGF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(dataset.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ThoBpdCY6B4P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = dataset.data\n",
        "y = dataset.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_Q_EQJsoDPd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(dataset.data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9lUSPGecpec7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset.target_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4aBr7a7M7WnM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One way to turn a text document into a feature vector is to use a frequency count of\n",
        "each word in the document.  We build a large dictionary of words, and for each document\n",
        "we return a vector with as many features as there are words, and for each word, we return\n",
        "the  number  of  times  that  word  appears  in  the  document  (this  is  technically  called\n",
        "**term frequency** , or tf for short).  Sklearn has a **[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)**  that does just that."
      ]
    },
    {
      "metadata": {
        "id": "xzguRZFUUv2h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TO DO: Transfrom the dataset into numerical feature vectors\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d-NxB5KHT579",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One problem with this representation is the high frequency of common words like ”the”\n",
        "or ”to” or ”and”.  Those words appear in almost all documents, so they don’t offer much information\n",
        "A better way to extract features from text is to use both the **term frequency** metric and the **inverse document frequency** metric . Sklearn has a **[TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)** that does just that."
      ]
    },
    {
      "metadata": {
        "id": "OP3srS8-7EYf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TO DO: Transfrom the dataset into tf-idf feature vectors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ukWHfO-7me2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you will need to use the vectorized dataset to perfom clustering.\n",
        "\n",
        "You will need to  use the following algorithms : [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html), [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.k_means.html#sklearn.cluster.k_means), [AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering).\n",
        "\n",
        "For each algorithm try to find the parameters that produce clusters as similar as possible to the real distribution of the data.\n",
        "\n",
        "Use different metrics to evaluate the algorithms : [Rand Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html),  [Silhouette Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), [Homogeneity Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html), [Completness Score.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Fs6scxqoRJpc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO : Use the following algorithms to perform clustering on the dataset  \n",
        "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score,homogeneity_score, completeness_score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmI2fRHhbBMq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, high dimensional sparse vectors do not produce the best clusters.\n",
        "Now, try to improve your results by using [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the dimensions of your feature vectors before applying the clustering algorithms. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fDLSqP34cB6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PteL66ZWmBsc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use t-SNE to produce a low-dimensional embedding of the dataset (and plot it)."
      ]
    },
    {
      "metadata": {
        "id": "vSXbJgJml8Zq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRsrJol6cmUO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As an extra exercise, try to implement kernel KMeans. Look at the KMeans course. Slide 70"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4mT5S8ZYcRxo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, _ = make_circles(n_samples=100, noise=0.1, factor=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4liSXptEdW0g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "\n",
        "k = 2\n",
        "# TODO : assign points to random clusters\n",
        "y = \n",
        "dist = np.zeros((X.shape[0], k))\n",
        "\n",
        "        \n",
        "# TODO\n",
        "max_iter = 10      \n",
        "for _ in range(max_iter):      \n",
        "  for j in range(k):\n",
        "    # TODO : get the points that are in cluster j\n",
        "    X_j = \n",
        "    \n",
        "    # TODO : compute the first term\n",
        "          \n",
        "    first_term = \n",
        "    \n",
        "    # TODO : compute the second term\n",
        "    \n",
        "    second_term = \n",
        "      \n",
        "    dist[:, j] = first_term + second_term\n",
        "        \n",
        "  # TODO : change the clusters\n",
        "  y = np.argmin(....)\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-MP7BYOCgjx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,0], X[:,1], c=y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}