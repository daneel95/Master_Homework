{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KNN_LAB_FINAL.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"F-5pHgs-V-ue"},"cell_type":"markdown","source":["# Lab 4. Classification  and regression using KNN and SVM\n","\n"]},{"metadata":{"colab_type":"code","id":"fhpgYh8o5hZi","colab":{}},"cell_type":"code","source":["# Some IPython magic\n","# Put these at the top of every notebook, here nbagg is used for interactive plots\n","%reload_ext autoreload\n","%autoreload 2\n","%matplotlib nbagg\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fsr9KBD_wcIL","colab_type":"text"},"cell_type":"markdown","source":["## KNN Classification\n","KNN is a non-parametric, instance-based, supervised learning algorithm.\n","\n","It doesn't learn a function, but memorizes the training set, and uses it at inference time.\n","\n","Pseudocode:\n","\n","- Train time: store all training points.\n","- Test time: find the closest k points , and output the most similar class.\n","### Implement your own knn\n","\n","Let's say you have the following dataset:"]},{"metadata":{"colab_type":"code","id":"81x8P1TI5hZk","colab":{}},"cell_type":"code","source":["# load data\n","# pickle is a python data file used to store objects on disk.\n","import pickle\n","data = pickle.load(open('data-knn.pkl', 'rb'))\n","X = data['data']\n","y = data['target']"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"SO0JfBYz5hZn","colab":{}},"cell_type":"code","source":["# Plot the dataset. The dataset is 2D.\n","plt.scatter(X[:,0], X[:,1], c=y)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DTdjYlENwcIZ","colab_type":"text"},"cell_type":"markdown","source":["Implement knn. Implement a function **`pairwise_distance_matrix(X,Y)`** in numpy, that computes the distance between any point in X with any point in Y. Try implementing this function with no for loops.\n","\n","Hint. You need to use numpy's broadcasting."]},{"metadata":{"colab_type":"code","id":"IyEl9bfy5hZt","colab":{}},"cell_type":"code","source":["# Implement your own euclidean distance method using numpy\n","\n","def pairwise_distance_matrix(X, Y):\n","    \"\"\"Compute the pairwise distance between rows of X and rows of Y\n","    Arguments\n","    ----------\n","    X: ndarray of size (N, D)\n","    Y: ndarray of size (M, D)\n","    Returns\n","    --------\n","    distances: matrix of shape (N, M), each entry D[i,j] is the distance between\n","    X[i,:] and Y[j,:] using the dot product.\n","    \"\"\"\n","    #########################\n","    # Compute distance_matrix\n","    #########################\n","    \n","    return distance_matrix\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"srdZcE5qwcIf","colab_type":"text"},"cell_type":"markdown","source":["Now implement KNN such that it takes as input the training dataset. Try to use numpy functions as much as possible."]},{"metadata":{"colab_type":"code","id":"qh1y_bu85hZx","colab":{}},"cell_type":"code","source":["# Implement your own version of KNN.\n","\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n","from sklearn.utils.multiclass import unique_labels\n","\n","class myKNN(BaseEstimator, ClassifierMixin):\n","    def __init__(self, n_neighbors=3):\n","        self.n_neighbors = n_neighbors\n","        \n","    def fit(self, X, y):\n","        # Check that X and y have correct shape\n","        X, y = check_X_y(X, y)\n","        # Store the classes seen during fit\n","        self.classes_ = unique_labels(y)\n","        self.X_ = X\n","        self.y_ = y\n","        # Return the classifier\n","        return self\n","    \n","    def predict(self, X):\n","        # Check is fit had been called\n","        check_is_fitted(self, ['X_', 'y_'])\n","        # Input validation\n","        X = check_array(X)\n","        # Implement knn predict\n","        \n","        return classes\n","    \n","    def predict_proba(self, X):\n","        return self.predict(X)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OvaYsL5YwcIm","colab_type":"text"},"cell_type":"markdown","source":["Test the accuracy of your implementation."]},{"metadata":{"colab_type":"code","id":"b8fxG6B_5hZz","colab":{}},"cell_type":"code","source":["# Find the accuracy of your KNN implementation\n","from sklearn.metrics import accuracy_score\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TdJt55RbwcIv","colab_type":"text"},"cell_type":"markdown","source":["Now train the sklearn KNN"]},{"metadata":{"colab_type":"code","id":"e23mH6o15hZ2","colab":{}},"cell_type":"code","source":["# Test sklearn's KNN implementation\n","# Hint! Use algorithm = 'brute' for first try\n","# Try to improve the score using other parameters for 'metric', 'algorithm' and 'n_neighbors'. \n","from sklearn.neighbors import KNeighborsClassifier\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H8QsC03-wcI0","colab_type":"text"},"cell_type":"markdown","source":["#### Finding the best k\n","k is a hyperparameter for knn.\n","\n","A hyperparameter is a parameter that is not learned from the data. In order to find the best hyperparameters you need to train, and measure the validation accuracy for multiple values of the hyperparameters."]},{"metadata":{"colab_type":"code","id":"gmTHgGmK5hZ6","colab":{}},"cell_type":"code","source":["# Implement a method that will find the best K parameter for a classifier.\n","# Plot every pair of score and K.\n","# Find best K parameter for the classifier declared before.\n","def find_best_k(clf, max_k=30):\n","    scores = []\n","    ks = np.arange(1,max_k+1)\n","    #############\n","    # train knn for diferent values of k and store the validation accuracy\n","\n","    plt.xticks(range(1,max_k+1))\n","    plt.plot(range(1,max_k+1), scores)\n","    \n","    max_score = max(scores)\n","    \n","    return scores.index(max_score) + 1, max_score\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AoX1Upw8wcI7","colab_type":"text"},"cell_type":"markdown","source":["Compare the two implementations of knn on the wine dataset. Don't forget to split the dataset into train and test sets."]},{"metadata":{"colab_type":"code","id":"oJCUpTtx5hZ9","colab":{}},"cell_type":"code","source":["# Load wine dataset and partition it in train and test splits.\n","from sklearn.datasets import load_wine\n","\n","wine = load_wine()\n","X = wine.data\n","y = wine.target\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"VTmDpyyJ5hZ_","colab":{}},"cell_type":"code","source":["# Test the accuracy of your KNN implementation and sklearn's KNN on wine dataset.\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SicsdaazwcJH","colab_type":"text"},"cell_type":"markdown","source":["Note that KNN relies on a distance measure. Therefore you need to normalize the data. Try the algorithms again after normalizing the data."]},{"metadata":{"colab_type":"code","id":"bw3wV4A45haJ","colab":{}},"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"C8dQNTLs5haL","colab":{}},"cell_type":"code","source":["## SVM Classification\n","\n","SVM is a classification algorithm that tries to find a separating hyperplane between two classes. It is therefore a linear algorithm. In order to obtain a non-linear separating plane, we can transform the feature space. This can be done using the kernel trick (see lecture slides)."],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"jUCt87mS5haO","colab":{}},"cell_type":"code","source":["# some datasets that we will use.\n","from sklearn import datasets\n","circles = datasets.make_circles(n_samples=200, factor=.5,\n","                                      noise=.05)\n","moons = datasets.make_moons(n_samples=200, noise=.05)\n","blobs = datasets.make_blobs(n_samples=200, random_state=9, centers=2, cluster_std=2)"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"9jyehwcAwcJU","colab_type":"code","colab":{}},"cell_type":"code","source":["datasets  = [circles, moons, blobs]\n","fig, axes = plt.subplots(3,1, figsize=(5,12))\n","for i, (X, y) in enumerate(datasets):\n","    axes[i].scatter(X[:,0],X[:,1],c=y)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dbqh2cDqwcJp","colab_type":"text"},"cell_type":"markdown","source":["Apply SVM to separate the blobs dataset and plot the decision boundary."]},{"metadata":{"id":"1P8Zw6VxwcJq","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.svm import SVC\n","clf = SVC(kernel='linear')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZFhqJC4HwcJx","colab_type":"text"},"cell_type":"markdown","source":["Now apply SVM on moons and circles datasets. Try different kernels. Which one works best?"]},{"metadata":{"id":"9j80RqX-wcJz","colab_type":"code","colab":{}},"cell_type":"code","source":["# apply svm for the datasets above"],"execution_count":0,"outputs":[]},{"metadata":{"id":"piS3Z12pwcJ1","colab_type":"text"},"cell_type":"markdown","source":["### Hyperparameter tuning\n","What if we had non-separable data? We need to find the best kernel with the best hyperparameters. We need to set up an experiment, split the dataset into train and test set, measure accuracy on test set to evaluate performance, and tune the hyperparameters to find the model that works best. "]},{"metadata":{"id":"qLjs1RzIwcJ5","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn import datasets\n","circles = datasets.make_circles(n_samples=200, factor=.5,\n","                                      noise=.25)\n","moons = datasets.make_moons(n_samples=200, noise=.25)\n","blobs = datasets.make_blobs(n_samples=200, random_state=3, centers=2, cluster_std=2.2)\n","datasets  = [circles, moons, blobs]\n","fig, axes = plt.subplots(3,1, figsize=(5,12))\n","for i, (X, y) in enumerate(datasets):\n","    axes[i].scatter(X[:,0],X[:,1],c=y)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oBA3-wRWwcKA","colab_type":"text"},"cell_type":"markdown","source":["#### Grid search\n","In order to tune a hyperparameter you should set up a validation experiment, and loop over different values of that hyperparameter, calling the experiment to train, test, score the model. If there are more hyperparameters, you could use nested loops. However, sklearn provides a tool to tune those parameters called [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Use grid search to find the best parameters for the 3 datasets (circles, moons and blobs)."]},{"metadata":{"id":"FCSSbXmRwcKB","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"yfGPz4RxwrSW","colab_type":"text"},"cell_type":"markdown","source":["## Regression\n","Both KNN and SVM can be adapted to be applied on regression problems. Both algorithms rely on distance metrics, therefore make sure you normalize your dataset. You can research different normalization techniques [here](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py).\n","\n","\n","To Do: a regression problem to on which to apply KNN and SVM regression"]},{"metadata":{"id":"25utFPUTwusk","colab_type":"code","colab":{}},"cell_type":"code","source":["# Apply KNN regression and SVM regression on the following dataset\n","dataset = fetch_california_housing()\n","X, y = dataset.data, dataset.target\n"],"execution_count":0,"outputs":[]}]}